{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting regional values from Met Office Global meteorological data\n",
    "\n",
    "## Process\n",
    "This notebook runs you through how to extract spatial mean values from gridded data using shapefiles. The process includes:\n",
    "1. Loading the gridded data from NetCDF files into memory using Iris (using [lazy loading](https://scitools.org.uk/iris/docs/latest/userguide/real_and_lazy_data.html)).\n",
    "2. Subset the global data to only include the USA, improving the processing time.\n",
    "3. Load the Shapefile for the regions we want to subset with.\n",
    "4. Define the functions to be used in the pipeline.\n",
    "5. Loop through all the regions in the shapefile; subsetting, collapsing and saving out to a CSV file for each region.\n",
    "6. Load all the region CSVs, collate into one large DataFrame and save out to CSV.\n",
    "\n",
    "## Method\n",
    "This process uses the polygon of a region (from the shapefile) to subset the gridded data by getting the **latitude-longitude bounding box** of the polygon, as described in this diagram:\n",
    "\n",
    "<img src=\"images/coarse_spatial_mean_gridded.png\" alt=\"Lat-Lon bounding box using polygon\" style=\"height: 400px;\"/> \n",
    "\n",
    "Each grid cell (small latitude-longitude box) contains a single value for a meteorological variable. The single value of that variable for the whole region/polygon is the mean of all the grid cell values in the bounding box i.e. lat-lon spatial mean.\n",
    "\n",
    "For example, here we have air temperature values in a bounding box that covers the a polygon. The temperature value for the region is the mean value of the temperatures in the boundind box - 20.9°C.\n",
    "\n",
    "<img src=\"images/spatial_mean_example.png\" alt=\"The mean value for the temperature is 20.9°C\" style=\"height: 400px;\"/> \n",
    "\n",
    "#### Time\n",
    "Of course we have ignored the time axis in this example, which is present in the gridded data but is handled for us by the Iris library as just another dimension. In this notebook we use daily data and will simply store the date for each value in the final tabular data.\n",
    "\n",
    "#### Improvements\n",
    "This process could be more accurate by only using the grid cells which actually overlap with the polygon and by weighting the grid cells according to how much of their area is within the polygon. Improvements like these are coming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "import iris\n",
    "import cartopy.io.shapereader as shpreader\n",
    "import pandas as pd\n",
    "import cftime\n",
    "\n",
    "#Plotting\n",
    "import iris.plot as iplt\n",
    "import iris.quickplot as qplt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#System\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "#Met Office utils\n",
    "import shape_utils as shape\n",
    "\n",
    "#Supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Met Office Global Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files for each variable are contained in a separate folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List all the filepaths and store in a dict with each variable as a key\n",
    "folder = '/data/covid19-ancillary-data/latest/mo_data_global_daily/'\n",
    "filepaths = {path: glob.glob(os.path.join(folder, path, '*.nc')) for path in os.listdir(folder)}\n",
    "variables = list(filepaths.keys())\n",
    "\n",
    "print(variables)\n",
    "print(f'Number of files for each variable: {len(filepaths[variables[0]])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Run through all the variables and append the loaded cubes to a CubeList\n",
    "cubes = iris.cube.CubeList([])\n",
    "\n",
    "for var in variables:\n",
    "    cubes.extend(iris.load(filepaths[var]))\n",
    "    \n",
    "print(cubes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Subset global data to the country we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset the cubes to just the USA\n",
    "us_latlon = ((18, 75), (-179, -65))\n",
    "us_cubes = iris.cube.CubeList([cube.intersection(latitude=us_latlon[0], longitude=us_latlon[1]) for cube in cubes])\n",
    "print(us_cubes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the subset to check that we have the right area\n",
    "qplt.contourf(us_cubes[0][0])\n",
    "plt.gca().coastlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the coordinate reference system from one of the cubes. We will use this later.\n",
    "CRS = us_cubes[0].coord_system()\n",
    "CRS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load shapefile containing region polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the shapefile\n",
    "shapefile = 'US_COUNTY_POP.shx'\n",
    "regions_reader = shpreader.Reader(shapefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many regions are included?\n",
    "len([record for record in regions_reader.records()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at one\n",
    "next(regions_reader.records())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(regions_reader.geometries())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use a list of the region IDs to loop through later\n",
    "region_ids = [record.attributes['OBJECTID'] for record in regions_reader.records()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(region_ids[0], region_ids[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to get the record from the reader\n",
    "def get_region_record(target, shapefile=regions_reader, attribute='OBJECTID'):\n",
    "    '''\n",
    "    Get the geometries for the specified target.\n",
    "    \n",
    "    '''\n",
    "    result = None\n",
    "    for record in shapefile.records():\n",
    "        location = record.attributes[attribute]\n",
    "        if location == target:\n",
    "            result = record\n",
    "            break\n",
    "    if result is None:\n",
    "        emsg = f'Could not find region with {attribute} \"{target}\".'\n",
    "        raise ValueError(emsg)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a random ID generator\n",
    "from random import randint\n",
    "def rand_id(ids=region_ids): \n",
    "    return randint(ids[0], ids[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a random geometry to check it's all working as expected\n",
    "get_region_record(regions_reader, rand_id()).geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define the functions to be used in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_name(cube):\n",
    "    name = cube.name()\n",
    "    method = cube.cell_methods[0].method.replace('imum', '')\n",
    "    units = cube.units\n",
    "    \n",
    "    if name == 'm01s01i202':\n",
    "        name = 'short_wave_radiation'\n",
    "        units = 'W/m2'\n",
    "    \n",
    "    return f'{name}_{method} ({units})'\n",
    "\n",
    "def get_date(dt):\n",
    "    if isinstance(dt, cftime.real_datetime):\n",
    "        date = dt.date()\n",
    "    else:\n",
    "        try:\n",
    "            date = datetime.datetime(dt.year, dt.month, dt.day).date()\n",
    "        except e:\n",
    "            raise Exception(e)\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(location, cubes=us_cubes, **kwargs):\n",
    "    region = get_region_record(location, **kwargs)\n",
    "    cutter = shape.Shape(region.geometry, region.attributes, coord_system=CRS)\n",
    "    cut_cubes = cutter.extract_subcubes(cubes)\n",
    "#     cubes_col = [cube.collapsed(['latitude','longitude'], iris.analysis.MEAN) for cube in cut_cubes]\n",
    "    return cut_cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_collapse_df(location, cubes=us_cubes, **kwargs):\n",
    "    region = get_region_record(location, **kwargs)\n",
    "    cutter = shape.Shape(region.geometry, region.attributes, coord_system=CRS)\n",
    "    cut_cubes = cutter.extract_subcubes(cubes)\n",
    "    cubes_col = [cube.collapsed(['latitude','longitude'], iris.analysis.MEAN) for cube in cut_cubes]\n",
    "    time = cubes_col[0].coord('time')\n",
    "    length = len(time.points)\n",
    "    data = {'objectid': [location]*length,\n",
    "            'fips': [region.attributes['FIPS']]*length,\n",
    "            'county_name': [region.attributes['NAME']]*length,\n",
    "            'state_name': [region.attributes['STATE_NAME']]*length,\n",
    "            'date': [get_date(cell.point) for cell in time.cells()]}\n",
    "    data.update({parse_data_name(cube): cube.data for cube in cubes_col})\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=COL_ORDER)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ = rand_id()\n",
    "print(id_)\n",
    "print(extract(id_, us_cubes, attribute='OBJECTID'))\n",
    "display(get_region_record(regions_reader, id_, 'OBJECTID').geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex = extract_collapse_df(id_)\n",
    "df_ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loop through all the regions in the shapefile; subsetting, collapsing and saving out to a CSV file for each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the order of the columns in the dataframes we will create\n",
    "col0 = ['objectid', 'fips', 'county_name', 'state_name', 'date']\n",
    "col1 = [parse_data_name(cube) for cube in us_cubes]\n",
    "COL_ORDER = tuple(col0 + sorted([c for c in col1 if c not in col0]))\n",
    "COL_ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at the cubes we are going to \n",
    "print(us_cubes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For now let's assume we haven't written any files, so we will loop through all the region IDs\n",
    "unwritten = region_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#This will loop through all the region IDs, executing extract_collapse_df for each region and saving it to a CSV file\n",
    "#Any errors will be caught and printed, but the loop will continue onto the next ID\n",
    "#Note that we cannot write to covid19-ancillary-data, so will have to write to /data/share/\n",
    "start = len(csvs)\n",
    "stop = len(region_ids)\n",
    "for location in unwritten[start:]:\n",
    "    try:\n",
    "        df = extract_collapse_df(location)\n",
    "        fname = df['fips'][0]\n",
    "        county = df['county_name'][0]\n",
    "        state = df['state_name'][0]\n",
    "        df.to_csv(f'/data/share/us_data/us_{fname}_daily_data_2020jan-mar.csv', index=False)\n",
    "        print(f'  [{location}] {fname}, {county}, {state}: Success')\n",
    "    except Exception as e:\n",
    "        print(f'x [{location}] {fname}, {county}, {state}: Error \\n  x  {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load all the region CSVs, collate into one large DataFrame and save out to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List all the csvs in /data/share/us_data/\n",
    "csvs = glob.glob('/data/share/us_data/*.csv')\n",
    "len(csvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Now load all the CSVs for each region and combine into one large dataframe\n",
    "df = pd.concat([pd.read_csv(csv) for csv in csvs], ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And save to a CSV\n",
    "fname_write = '/data/share/us_daily_precipdata_2020jan-mar_v01.csv'\n",
    "Mdf.to_csv(fname_write, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can read it back in to check that it wrote correctly\n",
    "pd.read_csv(fname_write)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
